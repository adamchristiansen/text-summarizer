"""
This module provides methods for loading and manipulating JSON documents
generated by the programs.
"""

import functools
import json
import operator

import numpy as np

from utils import text
from utils import weight

class Document:
    """
    Represents a document as a sequence of sentences.
    """

    def __init__(self, orig_article=None, orig_summary=None,
            sent_gen_summary=None, sent_orig_article=None,
            sent_orig_summary=None, title=None, topic=None):
        """
        Create an object consisting of the components of a document.

        # Arguments

        * `orig_article` (str): The original article.
        * `orig_summary` (str): The original summary.
        * `sent_gen_summary` (list<str>): The sentences in the generated
            summary.
        * `sent_orig_article` (list<str>): The sentences in the original
            article.
        * `sent_orig_summary` (list<str>): The sentences in the original
            summary.
        * `title` (str): The title of the article.
        * `topic` (str): The topic of the article.
        """
        self.orig_article      = orig_article
        self.orig_summary      = orig_summary
        self.sent_gen_summary  = sent_gen_summary
        self.sent_orig_article = sent_orig_article
        self.sent_orig_summary = sent_orig_summary
        self.title             = title
        self.topic             = topic

    @staticmethod
    def load_file(path):
        """
        Load a JSON document from a file.

        # Arguments

        * `path` (str): The path to the file.

        # Returns

        (Document): A document created using the specified JSON file.
        """
        with open(path) as f:
            return Document(**json.load(f))

    def dump_file(self, path):
        """
        Serialize as a JSON object.

        # Arguments

        * `path` (str): The path to the file to dump the JSON.
        """
        data = {
            'orig_article':      self.orig_article,
            'orig_summary':      self.orig_summary,
            'sent_gen_summary':  self.sent_gen_summary,
            'sent_orig_article': self.sent_orig_article,
            'sent_orig_summary': self.sent_orig_summary,
            'title':             self.title,
            'topic':             self.topic,
        }
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, sort_keys=True)

    def article_clean_word_sentences(self):
        """
        Clean the sentences in the article by breaking them into words and
        removing punctuation.

        # Returns

        (list<list<str>>): A list of sentences, where each sentence is a list
            of words.
        """
        cleaned = []
        for sentence in self.sent_orig_article:
            cleaned.append(text.split_words(sentence))
        return cleaned

    def article_sentence_word_occurrences(self):
        """
        Create a bin of word frequencies for every sentence in the document.

        # Returns

        (list<dict<str, int>>): A list of bins, where each bin represents a
        sentence in the order that they occur in the document. The bins map
        each word in the sentence to the number of times it occurs, therefore
        the value of each key-value pair in the bins is guaranteed to be at
        least 1.
        """
        sentence_bins = []
        for sentence in self.article_clean_word_sentences():
            bin = {}
            for word in sentence:
                if word not in bin:
                    bin[word] = 1
                else:
                    bin[word] += 1
            sentence_bins.append(bin)
        return sentence_bins

    def word_occurrences(self):
        """
        Count the occurences of every word in the cleaned document.

        # Returns

        (dict<str, int>): A dictionary whose keys are the cleaned words of the
        document and the values are the number of times each word occurs. Every
        value is at least one, so every key occurs in the document.
        """
        bins = {}
        s = self.article_clean_word_sentences()
        flat = functools.reduce(operator.iconcat, s, [])
        for word in flat:
            if word not in bins:
                bins[word] = 1
            else:
                bins[word] += 1
        return bins

    def word_weights(self, weight_func):
        """
        Create a weighted array of the word document word frequencies.

        # Arguments

        * `weight_func` (func<a> -> a where a is np.array<int>): A weighting
            function which accepts a term frequency array for a sentence and
            creates a new array by weighting each term.

        # Returns

        (numpy.matrix<float>): The assembled matrix.
        """
        # Word matrix without weighting or normalization constains all of the
        # word frequency information needed to build the document weight
        # vector.
        local_weight = weight.local_builder('none', normalize=False)
        return weight_func(self.word_matrix(local_weight))

    def word_matrix(self, weight_func):
        """
        Create a weighted matrix where the terms that occur in the document are
        the rows and the sentence word frequency vectors are the columns.

        # Arguments

        * `weight_func` (func<a> -> a where a is np.array<int>): A weighting
            function which accepts a term frequency array for a sentence and
            creates a new array by weighting each term.

        # Returns

        (numpy.matrix<float>): The assembled matrix.
        """
        # This is sorted for consistency, and so that a binary search can be
        # performed on it
        doc_words = sorted(self.word_occurrences().keys())
        def doc_words_index(word):
            """
            Given a word, perform a binary search on `doc_words` to find its
            index.

            # Arguments

            * `word` (str): The word to search for.

            # Returns

            (int): The index of the word in the array.

            # Raises

            * (ValueError): When the word is not found.
            """
            min_i = 0              # Inclusive
            max_i = len(doc_words) # Exclusive
            while min_i < max_i:
                mid_i = (min_i + max_i) // 2
                if doc_words[mid_i] == word:
                    return mid_i
                elif doc_words[mid_i] < word:
                    min_i = mid_i
                else:
                    max_i = mid_i
            # This will never be hit, but it is there just in case
            raise ValueError(f"Word '{word}' does not exist in document")
        sentence_bins = self.article_sentence_word_occurrences()
        matrix = np.zeros((len(doc_words), len(sentence_bins)))
        for col, bin in enumerate(sentence_bins):
            for word, count in bin.items():
                row = doc_words_index(word)
                matrix[row, col] = count
        # Apply the weighting
        for i in range(matrix.shape[1]):
            matrix[:,i] = weight_func(matrix[:,i])
        return matrix

    def set_summary(self, indices):
        """
        Set the generated summary based on sentence indices.

        # Arguments

        * `indices` (list<int>): The sentence indices to include in the
            summary.
        """
        self.sent_gen_summary = []
        for i in indices:
            self.sent_gen_summary.append(self.sent_orig_article[i])

    def summary_size(self):
        """
        The number of sentences in the original summary. This serves as the
        number of sentences to pick for the generated summary.

        # Returns

        (int): The number of sentences.
        """
        return len(self.sent_orig_summary)
